# Topic 7 Exercises: Nonlinear regression

## MÃ¡rcio Porto

## Programming

### 7.9.11

#### a)

```{r}
library(ISLR)
library(leaps)
attach(College)

set.seed(1)

train = sample(length(Outstate), length(Outstate) / 2)
test = -train
College.train = College[train,]
College.test = College[test,]

fit = regsubsets(Outstate ~ ., data=College.train, nvmax=17, method="forward")

plot(summary(fit)$cp, xlab = "# Variables", ylab = "Cp", type = "l")
min.cp <- min(summary(fit)$cp)
std.cp <- sd(summary(fit)$cp)
abline(h = min.cp + 0.2 * std.cp, col = "red", lty = 5)
abline(h = min.cp - 0.2 * std.cp, col = "red", lty = 5)

plot(summary(fit)$bic, xlab = "# Variables", ylab = "BIC", type='l')
min.bic <- min(summary(fit)$bic)
std.bic <- sd(summary(fit)$bic)
abline(h = min.bic + 0.2 * std.bic, col = "red", lty = 5)
abline(h = min.bic - 0.2 * std.bic, col = "red", lty = 5)

plot(summary(fit)$adjr2, xlab = "# Variables", ylab = "Adjusted R-squared", type = "l")
max.adjr2 <- max(summary(fit)$adjr2)
std.adjr2 <- sd(summary(fit)$adjr2)
abline(h = max.adjr2 + 0.2 * std.adjr2, col = "red", lty = 5)
abline(h = max.adjr2 - 0.2 * std.adjr2, col = "red", lty = 5)

fit = regsubsets(Outstate ~ ., data=College, method="forward")
coeffs = coef(fit, id=6)
names(coeffs)
```

For all three visualizations, 6 seems to be the ideal number of variables, since it's the lower number of variables inside the red lines.

#### b)

```{r}
library(gam)

fit = gam(Outstate ~ Private + s(Room.Board, df=2) + s(PhD, df=2) + s(perc.alumni, df=2) + s(Expend, df=5) + s(Grad.Rate, df=2), data=College.train)
plot(fit, se=T, col="red")
```

Expend seems to have a strong nonlinear relationship with the response variable. Grad.Rate and PhD also seem nonlinear. Room.Board and perc.alumni are pretty linear.

#### c)

```{r}
pred = predict(fit, College.test)
err = mean((College.test$Outstate - pred)^2)
err

tss = mean((College.test$Outstate - mean(College.test$Outstate))^2)
rss = 1 - err / tss
rss
```

Test RSS was 0.7696916 for this GAM, which indicates it is a good model of this relationship.

#### d)

```{r}
summary(fit)
```

Expend seems to have a very strong nonlinear relationship with Outstate. There's also some evidence for PhD.

## Theory

### 7.9.3

```{r}
x = -2:2
y = 1 + x + -2*((x-1)^2 * I(x>1))
plot(x, y)
```

The fit is initially linear, then becomes quadratic at the top of the plot. The intercept is 1 for both formulas.

### 7.9.4

```{r}
x = -2:2
y = c(1 + (0 - 0) + (3 * (0 + 0)),
      1 + (0 - 0) + (3 * (0 + 0)),
      1 + (1 - 0) + (3 * (0 + 0)),
      1 + (1 - 0) + (3 * (0 + 0)),
      1 + (1 - 1) + (3 * (0 + 0)))
plot(x, y)
```

At first the fit is constant at the bottom of the plot, then becomes constant at the top of the plot, and ends up being negatively linear towards the right side of the plot.

### 7.9.5

#### a)

g2 will be a higher order polynomial because it is more flexible. So it will have the smaller training RSS.

#### b)

Due to a), g2 will likely overfit the data, which means that g1 will probably have the smaller test RSS.

#### c)

At lambda = 0, g1 and g2 will be the same.
